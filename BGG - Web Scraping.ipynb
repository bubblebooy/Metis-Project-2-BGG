{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:45.548435Z",
     "start_time": "2020-06-30T15:11:45.542437Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# https://github.com/bubblebooy/chi20_ds14/blob/master/curriculum/project-02/web-scraping-beautifulsoup/web_scraping_beautifulsoup.ipynb\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time, os\n",
    "\n",
    "# https://github.com/bubblebooy/chi20_ds14/blob/master/curriculum/project-02/web-scraping-selenium/web_scraping_selenium.ipynb\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver = \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:47.357650Z",
     "start_time": "2020-06-30T15:11:47.352652Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_browse(page = 1):\n",
    "    \"\"\"\n",
    "    Get scrapes data from BBG browse with specified page.\n",
    "    browse page is sorted by descending number of votes\n",
    "    \"\"\"\n",
    "    url = f'https://boardgamegeek.com/browse/boardgame/page/{page}?sort=numvoters&sortdir=desc' \n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    return BeautifulSoup(page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:47.767841Z",
     "start_time": "2020-06-30T15:11:47.763842Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_column_str(col):\n",
    "    \"\"\"\n",
    "    removes ( tabs and newlines ) and strips strings in a column\n",
    "    \"\"\"    \n",
    "    return col.str.replace(\"\\t\",\"\").str.replace(\"\\n\",\"\").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:48.237372Z",
     "start_time": "2020-06-30T15:11:48.234373Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_get_text(soup_list):\n",
    "    \"\"\"\n",
    "    Gets the textContent from each item in a list of html elements. Returns a list\n",
    "    \"\"\"\n",
    "    return [item.text for item in soup_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:48.754330Z",
     "start_time": "2020-06-30T15:11:48.742334Z"
    }
   },
   "outputs": [],
   "source": [
    "def browse_html_to_df(soup):\n",
    "    \"\"\"\n",
    "    Takes the html from a BBG browse page and return the relevant information as a DataFrame\n",
    "    \"\"\"\n",
    "    _df = pd.DataFrame()\n",
    "    _df[\"title\"] = [ element.text for element in soup.select(\".collection_objectname a\") ]\n",
    "    _df[\"link\"] = [ element.get(\"href\") for element in soup.select(\".collection_objectname a\") ]\n",
    "    _df[\"date\"] = [ element.text for element in soup.select(\".collection_objectname span\") ]\n",
    "    _df[\"BGG_rank\"] = [ element.text for element in soup.select(\".collection_rank\") ]  # so many tabs\n",
    "    _df[\"geek_rating\"] = [ element.select(\".collection_bggrating\")[0].text for element in soup.select(\"tr#row_\") ]\n",
    "    _df[\"avg_rating\"] = [ element.select(\".collection_bggrating\")[1].text for element in soup.select(\"tr#row_\") ]\n",
    "    _df[\"num_votes\"] = [ element.select(\".collection_bggrating\")[2].text for element in soup.select(\"tr#row_\") ]\n",
    "\n",
    "    _df[\"date\"] = _df[\"date\"].str.replace(\"(\",\"\").str.replace(\")\",\"\")\n",
    "    _df[\"BGG_rank\"] = clean_column_str(_df[\"BGG_rank\"])\n",
    "    _df[\"geek_rating\"] = clean_column_str(_df[\"geek_rating\"])\n",
    "    _df[\"avg_rating\"] = clean_column_str(_df[\"avg_rating\"])\n",
    "    _df[\"num_votes\"] = clean_column_str(_df[\"num_votes\"])\n",
    "    _df = _df[_df.BGG_rank != 'N/A']    #games with no rank will be dropped / these are expansions\n",
    "    _df = _df.dropna()                  \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:11:49.403196Z",
     "start_time": "2020-06-30T15:11:49.380203Z"
    }
   },
   "outputs": [],
   "source": [
    "def game_page_info(_df):\n",
    "    \"\"\"\n",
    "    Takes a Datafram w/ information from a BBG browse page and scrapes additional information for each game in the DataFrame from the games BGG game page\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    for i,link in enumerate(tqdm(_df[\"link\"], leave  = False)):\n",
    "        url = 'https://boardgamegeek.com' + link + '/credits'\n",
    "        driver.get(url)\n",
    "        time.sleep(.5 + np.random.random()*5 )\n",
    "        soup_game_page      = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        _df.at[i,\"players\"]  =soup_game_page.find_all(class_ = \"gameplay-item-primary\")[0].find('span').text\n",
    "        _df.at[i,\"play_time\"]=soup_game_page.find_all(class_ = \"gameplay-item-primary\")[1].find('span').text\n",
    "        _df.at[i,\"age\"]      =soup_game_page.find_all(class_ = \"gameplay-item-primary\")[2].find('span').text\n",
    "\n",
    "        max_wait_time = 5.0\n",
    "        while max_wait_time:\n",
    "            try:\n",
    "                full_credits = soup_game_page.find_all(class_ = \"global-body-content-primary\")[1]\n",
    "                designer_list =  list_get_text(full_credits.find_all(class_ = 'outline-item-description')[3].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\"))\n",
    "                artists_list =   list_get_text(full_credits.find_all(class_ = 'outline-item-description')[4].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\"))\n",
    "                publishers_list = list_get_text(full_credits.find_all(class_ = 'outline-item-description')[5].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\"))  \n",
    "                categories_list = list_get_text(full_credits.find_all(class_ = 'outline-item-description')[6].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\"))  \n",
    "                mechanisms_list =   list_get_text(full_credits.find_all(class_ = 'outline-item-description')[7].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\"))\n",
    "                family_list = list_get_text(full_credits.find_all(class_ = 'outline-item-description')[8].find(class_ = \"ng-scope\").find_all(class_ = \"ng-scope\")) \n",
    "                break\n",
    "            except:\n",
    "                driver.get(url)\n",
    "                soup_game_page = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                print(f'{url} not loaded waiting : {max_wait_time} seconds')\n",
    "                designer_list = [\"\"]\n",
    "                artists_list = [\"\"]\n",
    "                publishers_list = [\"\"]\n",
    "                categories_list = [\"\"]\n",
    "                mechanisms_list = [\"\"]\n",
    "                family_list = [\"\"]\n",
    "                time.sleep(.5)\n",
    "                max_wait_time -= .5\n",
    "\n",
    "        _df.at[i, \"designer\"]   = \" , \".join(designer_list)    \n",
    "        _df.at[i, \"artists\"]    = \" , \".join(artists_list)    \n",
    "        _df.at[i, \"publishers\"] = \" , \".join(publishers_list)    \n",
    "        _df.at[i, \"categories\"] = \" , \".join(categories_list)    \n",
    "        _df.at[i, \"mechanisms\"] = \" , \".join(mechanisms_list)    \n",
    "        _df.at[i, \"family\"]     = \" , \".join(family_list)\n",
    "\n",
    "    driver.quit()\n",
    "    _df[\"players\"] = clean_column_str(_df[\"players\"])\n",
    "    _df[\"play_time\"] = clean_column_str(_df[\"play_time\"])\n",
    "    _df[\"age\"] = clean_column_str(_df[\"age\"])\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:27:05.348834Z",
     "start_time": "2020-06-30T15:11:51.813121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c146cbd1a2846168fe6a9fe6d4328a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200, 16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set to True for web scraping otherwise read pickle\n",
    "pkl_name = \"./bbg_df.pkl\"\n",
    "\n",
    "\n",
    "#Set True to concat web scraping to pickle\n",
    "if False:\n",
    "    df = pd.read_pickle(pkl_name)\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "for page in tqdm(range(0,21)):  #page 21 does not work. random pages after also fail.\n",
    "    soup = scrape_browse(page)\n",
    "    df = pd.concat([df , game_page_info(browse_html_to_df(soup))])\n",
    "    df.to_pickle(pkl_name)\n",
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
